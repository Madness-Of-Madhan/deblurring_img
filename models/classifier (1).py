# -*- coding: utf-8 -*-
"""classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BYsNZJgpKW9csJlWIQ5eVbtEDuyFtmNF

# Blur Type Classification using DIV2K Dataset
## Classifying images into Sharp, Gaussian Blur, and Motion Blur categories
"""

import kagglehub
import os
import cv2
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2 # Import for base_model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D # Import for custom layers
from tensorflow.keras.models import Model, load_model # Import for model loading
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from PIL import Image
import random
from tqdm import tqdm
import seaborn as sns
import json
import warnings
import shutil # For clearing directories

warnings.filterwarnings('ignore')

# Cell 1: Install and Import Dependencies (already executed in previous notebook state, ensuring libraries are available)
# No need to reinstall, just ensure imports are here for completeness if running as a standalone script.

# Re-initialize base_dir
base_dir = "blur_classification"

# Re-initialize path from the dataset download in Cell 2
# This is a critical assumption that 'path' is correctly set and its contents are available
path_from_cell2 = '/root/.cache/kagglehub/datasets/soumikrakshit/div2k-high-resolution-images/versions/1'
current_download_path = path_from_cell2 # Start with the assumed path

# Check if the assumed dataset path exists, if not, re-download the dataset
if not os.path.exists(current_download_path) or not os.path.isdir(current_download_path):
    print(f"Dataset path {current_download_path} not found. Attempting to re-download DIV2K dataset...")
    try:
        current_download_path = kagglehub.dataset_download("soumikrakshit/div2k-high-resolution-images")
        print("Dataset re-downloaded to:", current_download_path)
    except Exception as e:
        print(f"Error re-downloading dataset: {e}. Please ensure kagglehub is correctly installed and configured.")
        raise
else:
    print(f"Dataset found at: {current_download_path}")

path = current_download_path # Use the confirmed or re-downloaded path

# Debugging: List contents of the downloaded dataset path
print(f"Contents of downloaded dataset path ({path}): {os.listdir(path)}")

# Define target directories and ensure they are clean and created
dirs = {
    'train_sharp': f"{base_dir}/train/sharp",
    'train_gaussian': f"{base_dir}/train/gaussian",
    'train_motion': f"{base_dir}/train/motion",
    'val_sharp': f"{base_dir}/val/sharp",
    'val_gaussian': f"{base_dir}/val/gaussian",
    'val_motion': f"{base_dir}/val/motion",
    'models': f"{base_dir}/models",
    'outputs': f"{base_dir}/outputs"
}

# Clear and re-create all target directories to ensure a clean state
if os.path.exists(base_dir):
    print(f"Clearing existing directory: {base_dir}")
    shutil.rmtree(base_dir) # Remove base_dir and all its contents
os.makedirs(base_dir, exist_ok=True) # Re-create base_dir

for d in dirs.values():
    os.makedirs(d, exist_ok=True)
print("Directories cleared and re-created.")


# Cell 3: Blur Generation Functions
def apply_gaussian_blur(image_path, output_path, kernel_size=(15, 15), sigma=2.0):
    """Apply Gaussian blur to an image"""
    img = cv2.imread(image_path)
    if img is None:
        return None
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    blurred = cv2.GaussianBlur(img, kernel_size, sigma)
    Image.fromarray(blurred).save(output_path)
    return blurred

def apply_motion_blur(image_path, output_path, size=15, angle=45):
    """Apply motion blur to an image"""
    img = cv2.imread(image_path)
    if img is None:
        return None
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Create motion blur kernel
    kernel = np.zeros((size, size))
    kernel[int((size-1)/2), :] = np.ones(size)
    # Correctly define M before use
    M = cv2.getRotationMatrix2D((size/2, size/2), angle, 1)
    kernel = cv2.warpAffine(kernel, M, (size, size))

    blurred = cv2.filter2D(img, -1, kernel)
    Image.fromarray(blurred).save(output_path)
    return blurred

# Cell 4: Process Dataset and Create Blurred Images
def process_dataset(source_dir, target_base, num_images=50):
    """Process images and create blurred versions"""
    if not os.path.isdir(source_dir):
        print(f"Error: Source directory not found or not a directory: {source_dir}")
        return # Exit early if source_dir is invalid

    image_files = [f for f in os.listdir(source_dir)
                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:num_images]

    print(f"Processing {len(image_files)} images from {source_dir}...")

    os.makedirs(os.path.join(target_base, 'sharp'), exist_ok=True)
    os.makedirs(os.path.join(target_base, 'gaussian'), exist_ok=True)
    os.makedirs(os.path.join(target_base, 'motion'), exist_ok=True)

    for img_file in tqdm(image_files):
        base_name = os.path.splitext(img_file)[0]
        src_path = os.path.join(source_dir, img_file)

        img_raw = cv2.imread(src_path)
        if img_raw is None:
            print(f"Warning: Could not read image {src_path}, skipping.")
            continue

        sharp_path = os.path.join(target_base, 'sharp', f"{base_name}.png")
        img = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)
        Image.fromarray(img).save(sharp_path)

        gaussian_path = os.path.join(target_base, 'gaussian', f"{base_name}.png")
        apply_gaussian_blur(src_path, gaussian_path)

        motion_path = os.path.join(target_base, 'motion', f"{base_name}.png")
        apply_motion_blur(src_path, motion_path, angle=random.randint(0, 180))

# Process training data with corrected DOUBLE-nested path (as per previous successful Cell 8fdd8c3a and debugging)
train_source = os.path.join(path, "DIV2K_train_HR", "DIV2K_train_HR")
print(f"Checking train_source: {train_source}, exists: {os.path.exists(train_source)}")
if os.path.exists(train_source):
    process_dataset(train_source, f"{base_dir}/train", num_images=100)
else:
    print(f"CRITICAL ERROR: Training source directory not found at {train_source}. Dataset processing failed.")

# Process validation data with corrected DOUBLE-nested path (as per previous successful Cell 8fdd8c3a and debugging)
val_source = os.path.join(path, "DIV2K_valid_HR", "DIV2K_valid_HR")
print(f"Checking val_source: {val_source}, exists: {os.path.exists(val_source)}")
if os.path.exists(val_source):
    process_dataset(val_source, f"{base_dir}/val", num_images=30)
else:
    print(f"CRITICAL ERROR: Validation source directory not found at {val_source}. Dataset processing failed.")

print("Dataset preparation complete!")

# Cell 5: Data Generator
class BlurTypeDataGenerator(keras.utils.Sequence):
    def __init__(self, data_dir, batch_size=32, img_size=(224, 224), shuffle=True):
        self.batch_size = batch_size
        self.img_size = img_size
        self.shuffle = shuffle

        self.classes = ['sharp', 'gaussian', 'motion']
        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}

        self.image_paths = []
        self.labels = []

        for class_name in self.classes:
            class_dir = os.path.join(data_dir, class_name)
            if os.path.exists(class_dir):
                for img_file in os.listdir(class_dir):
                    if img_file.endswith(('.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG')):
                        self.image_paths.append(os.path.join(class_dir, img_file))
                        self.labels.append(self.class_to_idx[class_name])
            else:
                print(f"Warning: Directory not found for class {class_name} at {class_dir}")

        self.indices = np.arange(len(self.image_paths))
        self.on_epoch_end()
        print(f"Initialized DataGenerator with {len(self.image_paths)} images from {data_dir}")

    def __len__(self):
        return int(np.ceil(len(self.image_paths) / self.batch_size))

    def __getitem__(self, index):
        start_idx = index * self.batch_size
        end_idx = min((index + 1) * self.batch_size, len(self.image_paths))
        batch_indices = self.indices[start_idx:end_idx]

        batch_images = []
        batch_labels = []

        for idx in batch_indices:
            try:
                img = tf.keras.preprocessing.image.load_img(
                    self.image_paths[idx], target_size=self.img_size
                )
                img = tf.keras.preprocessing.image.img_to_array(img)
                img = img / 255.0
                batch_images.append(img)
                batch_labels.append(self.labels[idx])
            except Exception as e:
                print(f"Error loading image {self.image_paths[idx]}: {e}")
                continue

        return np.array(batch_images), np.array(batch_labels)

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.indices)

# Cell 6: Create Data Generators
train_generator = BlurTypeDataGenerator(
    data_dir=f"{base_dir}/train",
    batch_size=32,
    img_size=(224, 224)
)

val_generator = BlurTypeDataGenerator(
    data_dir=f"{base_dir}/val",
    batch_size=32,
    img_size=(224, 224)
)

print(f"Training samples: {len(train_generator.image_paths)}")
print(f"Validation samples: {len(val_generator.image_paths)}")
print(f"Classes: {train_generator.classes}")

# Cell 16 & 17 (from original notebook context for transfer learning model definition)
# --- Model Re-definition ---
# 1. Re-create the base_model (MobileNetV2 from Cell 18b575c8)
base_model = MobileNetV2(input_shape=(224, 224, 3),
                         include_top=False,
                         weights='imagenet')
base_model.trainable = True # Set to True for fine-tuning later

# 2. Re-create the full model architecture (from Cell 78898429)
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dense(3, activation='softmax')(x) # 3 classes: sharp, gaussian, motion

model = Model(inputs=base_model.input, outputs=x)

# Cell 9: Callbacks (re-define for consistency)
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath=f"{base_dir}/models/best_model.h5", # This will be the best model during the initial unfrozen training
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=5,
        min_lr=1e-7,
        verbose=1
    )
]

# Check if a fine_tuned_model.h5 exists, otherwise train from scratch
fine_tuned_model_path = os.path.join(base_dir, "models", "fine_tuned_model.h5")
if os.path.exists(fine_tuned_model_path):
    print(f"Loading fine-tuned model from {fine_tuned_model_path}...")
    model = load_model(fine_tuned_model_path)
    print("Fine-tuned model loaded successfully for further evaluation/training.")
else:
    print(f"Fine-tuned model not found at {fine_tuned_model_path}. Training initial model...")
    # Cell 8: Compile Model (initial unfrozen training)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    print("Model compiled for initial training.")

    # Cell 10: Train Model (initial training)
    history = model.fit(
        train_generator,
        validation_data=val_generator,
        epochs=30, # Increased epochs for better initial training
        callbacks=callbacks,
        verbose=1
    )
    model.save(f"{base_dir}/models/initial_trained_model.h5")
    print("Initial model training completed and saved!")

    # Load the best model saved during initial training for subsequent steps
    if os.path.exists(f"{base_dir}/models/best_model.h5"):
        model = load_model(f"{base_dir}/models/best_model.h5")
        print("Best model from initial training loaded.")
    else:
        print("Warning: No best_model.h5 found from initial training. Using the last epoch's model.")

# Cell 18: Fine-tune (Optional)
print("\nProceeding to fine-tune the model...")
base_model.trainable = True # Unfreeze layers for fine-tuning
# Recompile model with a very low learning rate for fine-tuning
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-5), # Very low learning rate for fine-tuning
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
print("Model recompiled for fine-tuning with a low learning rate.")

# Create new callbacks for fine-tuning
callbacks_fine_tune = [
    keras.callbacks.ModelCheckpoint(
        filepath=f"{base_dir}/models/fine_tuned_best.h5",
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True,
        verbose=1
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-8,
        verbose=1
    )
]

history_fine_tune = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10, # Additional epochs for fine-tuning
    callbacks=callbacks_fine_tune,
    verbose=1
)

# Save the fine-tuned model
model.save(fine_tuned_model_path)
print("Fine-tuning completed and model saved!")

# Cell 11: Plot Training History (Fine-tuned model)
print("\nPlotting fine-tuned model training history...")
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Plot accuracy
axes[0].plot(history_fine_tune.history['accuracy'], label='Train Accuracy')
axes[0].plot(history_fine_tune.history['val_accuracy'], label='Val Accuracy')
axes[0].set_title('Fine-tuned Model Accuracy')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend()
axes[0].grid(True)

# Plot loss
axes[1].plot(history_fine_tune.history['loss'], label='Train Loss')
axes[1].plot(history_fine_tune.history['val_loss'], label='Val Loss')
axes[1].set_title('Fine-tuned Model Loss')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.savefig(f"{base_dir}/outputs/fine_tune_training_history.png")
plt.show()
print("Fine-tuned training history plot saved.")

# Cell 12: Evaluate Model (Fine-tuned model)
print("\nEvaluating fine-tuned model...")
y_true = []
y_pred = []

if len(val_generator.image_paths) > 0:
    for i in range(len(val_generator)):
        X, y = val_generator[i]
        if len(X) == 0:
            continue
        pred = model.predict(X, verbose=0)
        y_true.extend(y)
        y_pred.extend(np.argmax(pred, axis=1))

if not y_true or not y_pred:
    print("Error: No samples found for evaluation after fine-tuning. y_true or y_pred is empty. Cannot generate reports/plots.")
else:
    # Classification report
    print("\nClassification Report (Fine-tuned Model):")
    print(classification_report(y_true, y_pred, target_names=['Sharp', 'Gaussian', 'Motion']))

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Sharp', 'Gaussian', 'Motion'],
                yticklabels=['Sharp', 'Gaussian', 'Motion'])
    plt.title('Fine-tuned Model Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig(f"{base_dir}/outputs/fine_tune_confusion_matrix.png")
    plt.show()
    print("Fine-tuned confusion matrix plot saved.")

# Cell 13: Test on Random Images (Fine-tuned model)
print("\nTesting fine-tuned model on random images...")
def predict_blur_type(image_path, model, img_size=(224, 224)):
    """Predict blur type for a single image"""
    img = Image.open(image_path)
    img_resized = img.resize(img_size)
    img_array = np.array(img_resized) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    pred = model.predict(img_array, verbose=0)
    pred_class = np.argmax(pred[0])
    confidence = pred[0][pred_class]

    class_names = ['Sharp', 'Gaussian Blur', 'Motion Blur']
    return class_names[pred_class], confidence, img

test_images = []
for class_name in ['sharp', 'gaussian', 'motion']:
    class_dir = f"{base_dir}/val/{class_name}"
    if os.path.exists(class_dir):
        images = os.listdir(class_dir)[:2]
        for img_file in images:
            test_images.append(os.path.join(class_dir, img_file))

if test_images:
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    for i, img_path in enumerate(test_images[:6]):
        pred_class, confidence, img_display = predict_blur_type(img_path, model)

        true_class = img_path.split('/')[-2]
        true_class_name = {'sharp': 'Sharp', 'gaussian': 'Gaussian', 'motion': 'Motion'}[true_class]

        axes[i].imshow(img_display)
        color = 'green' if pred_class.lower().startswith(true_class) else 'red'
        title = f"True: {true_class_name}\nPred: {pred_class}\nConf: {confidence:.2f}"
        axes[i].set_title(title, color=color)
        axes[i].axis('off')

    plt.tight_layout()
    plt.savefig(f"{base_dir}/outputs/fine_tune_test_predictions.png")
    plt.show()
    print("Fine-tuned test predictions plot saved.")
else:
    print("No test images available to display predictions.")

# Cell 14: Save Results and Download Model (Fine-tuned model)
print("\nSaving final evaluation results...")
eval_results = {
    'final_train_accuracy': float(history_fine_tune.history['accuracy'][-1]),
    'final_val_accuracy': float(history_fine_tune.history['val_accuracy'][-1]),
    'final_train_loss': float(history_fine_tune.history['loss'][-1]),
    'final_val_loss': float(history_fine_tune.history['val_loss'][-1]),
    'best_val_accuracy': float(max(history_fine_tune.history['val_accuracy'])),
    'classes': ['Sharp', 'Gaussian Blur', 'Motion Blur'],
    'fine_tuned_classification_report': classification_report(y_true, y_pred, target_names=['Sharp', 'Gaussian', 'Motion'], output_dict=True) if y_true else "N/A"
}

with open(f"{base_dir}/outputs/fine_tune_evaluation_results.json", 'w') as f:
    json.dump(eval_results, f, indent=2)

print("Evaluation Results (Fine-tuned Model):")
print(json.dumps(eval_results, indent=2))

try:
    from google.colab import files
    # Only download the fine-tuned best model
    if os.path.exists(f"{base_dir}/models/fine_tuned_best.h5"):
        files.download(f"{base_dir}/models/fine_tuned_best.h5")
        print("Fine-tuned best model downloaded successfully!")
    else:
        print(f"Warning: fine_tuned_best.h5 not found at {base_dir}/models/. No model downloaded.")
except ImportError:
    print("Not in Colab environment. Fine-tuned best model saved locally at:", f"{base_dir}/models/fine_tuned_best.h5")

print("\n--- Comprehensive Fix and Re-evaluation Complete ---")
print("Summary of fine-tuned model performance:")
if y_true:
    report = classification_report(y_true, y_pred, target_names=['Sharp', 'Gaussian', 'Motion'], output_dict=True)
    print(f"Accuracy: {report['accuracy']:.2f}")
    print(f"Macro Avg F1-score: {report['macro avg']['f1-score']:.2f}")
    print(f"Weighted Avg F1-score: {report['weighted avg']['f1-score']:.2f}")
else:
    print("No evaluation metrics could be generated due to empty dataset.")

"""**Reasoning**:
The previous code block executed successfully, completing the comprehensive fix and re-evaluation. All steps, including dataset processing, model training, fine-tuning, and evaluation, ran without errors, and the final results are presented. Therefore, the task is complete, and no further steps are required.


"""

# This is how Cell 4 should look like if there's an extra nesting level.
# Please execute the cell above to determine the correct path, then modify Cell 4.

# Cell 4: Process Dataset and Create Blurred Images
def process_dataset(source_dir, target_base, num_images=50):
    """Process images and create blurred versions"""
    image_files = [f for f in os.listdir(source_dir)
                   if f.endswith(('.png', '.jpg', '.jpeg'))][:num_images]

    print(f"Processing {len(image_files)} images from {source_dir}...")

    for img_file in tqdm(image_files):
        base_name = os.path.splitext(img_file)[0]
        src_path = os.path.join(source_dir, img_file)

        # Save sharp image
        sharp_path = os.path.join(target_base, 'sharp', f"{base_name}.png")
        img = cv2.imread(src_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        Image.fromarray(img).save(sharp_path)

        # Save Gaussian blur
        gaussian_path = os.path.join(target_base, 'gaussian', f"{base_name}.png")
        apply_gaussian_blur(src_path, gaussian_path)

        # Save Motion blur (multiple angles for variety)
        motion_path = os.path.join(target_base, 'motion', f"{base_name}.png")
        apply_motion_blur(src_path, motion_path, angle=random.randint(0, 180))

# Process training data
# Adjust 'DIV2K_train_HR/DIV2K_train_HR' if the directory structure is different
train_source = os.path.join(path, "DIV2K_train_HR", "DIV2K_train_HR") # Likely fix
if os.path.exists(train_source):
    process_dataset(train_source, f"{base_dir}/train", num_images=100)
else:
    print(f"Warning: Training source directory not found at {train_source}. Check dataset extraction.")

# Process validation data
# Adjust 'DIV2K_valid_HR/DIV2K_valid_HR' if the directory structure is different
val_source = os.path.join(path, "DIV2K_valid_HR", "DIV2K_valid_HR") # Likely fix
if os.path.exists(val_source):
    process_dataset(val_source, f"{base_dir}/val", num_images=30)
else:
    print(f"Warning: Validation source directory not found at {val_source}. Check dataset extraction.")

print("Dataset preparation complete!")